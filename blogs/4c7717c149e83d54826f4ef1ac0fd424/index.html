<!doctype html><html lang="ja"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><meta name="description" content="生成AI・LLM関連の取り組みをしている会社・個人のテックブログの更新をまとめたRSSフィードを配信しています。記事を読んでその企業の技術・カルチャーを知れることや、質の高い技術情報を得られることを目的としています。"><meta name="author" content="r-kagaya"><meta name="robots" content="index, follow"><meta property="og:url" content="https://r-kagaya.github.io/llm-tech-blog-rss-feed/"><meta property="og:title" content="Zennの「LLM」のフィードのフィード｜生成AI・LLM関連テックブログRSS"><meta property="og:image" content="https://r-kagaya.github.io/llm-tech-blog-rss-feed/images/og-image.png"><meta property="og:description" content="生成AI・LLM関連の取り組みをしている会社・個人のテックブログの更新をまとめたRSSフィードを配信しています。記事を読んでその企業の技術・カルチャーを知れることや、質の高い技術情報を得られることを目的としています。"><meta property="og:type" content="website"><meta property="og:site_name" content="生成AI・LLM関連テックブログRSS"><meta name="twitter:card" content="summary"><meta property="twitter:domain" 
content="https://r-kagaya.github.io/llm-tech-blog-rss-feed/"><meta property="twitter:url" content="https://r-kagaya.github.io/llm-tech-blog-rss-feed/"><meta name="twitter:title" content="Zennの「LLM」のフィードのフィード｜生成AI・LLM関連テックブログRSS"><meta name="twitter:description" content="生成AI・LLM関連の取り組みをしている会社・個人のテックブログの更新をまとめたRSSフィードを配信しています。記事を読んでその企業の技術・カルチャーを知れることや、質の高い技術情報を得られることを目的としています。"><meta name="twitter:image" content="https://r-kagaya.github.io/llm-tech-blog-rss-feed/images/og-image.png"><link rel="shortcut icon" href="../../images/favicon.ico"><link rel="apple-touch-icon" href="../../images/apple-icon.png"><link rel="alternate" type="application/atom+xml" title="Atom Feed" href="../../feeds/atom.xml"><link rel="alternate" type="application/rss+xml" title="RSS2.0" href="../../feeds/rss.xml"><link rel="alternate" type="application/json" href="../../feeds/feed.json"><style>
*,::after,::before{box-sizing:border-box}*{margin:0}body,html{height:100%}body{line-height:1.5;-webkit-font-smoothing:antialiased}canvas,img,picture,svg,video{display:block;max-width:100%}button,input,select,textarea{font:inherit}h1,h2,h3,h4,h5,h6,p{overflow-wrap:break-word}#__next,#root{isolation:isolate}:root{--ui-color-brand:#353535;--ui-color-n-000:#fff;--ui-color-n-100:#ebebeb;--ui-color-n-300:#aeaeae;--ui-color-n-500:#353535;--ui-color-n-700:#282828;--ui-color-n-900:#1a1a1a;--ui-color-background-primary:var(--ui-color-n-000);--ui-color-form-input:var(--ui-color-n-100);--ui-color-typography-heading:var(--ui-color-n-500);--ui-color-typography-body:var(--ui-color-n-900);--ui-color-typography-note:var(--ui-color-n-300);--ui-color-typography-button:var(--ui-color-n-000);--ui-typography-typeface:"Inter",sans-serif;--ui-typography-h1:1.9375rem;--ui-typography-h2:1.5625rem;--ui-typography-h3:1.25rem;--ui-typography-p:1rem;--ui-typography-s:.8125rem;--ui-typography-h1-leading:1.2;--ui-typography-h2-leading:1.2;--ui-typography-h3-leading:1.25;--ui-typography-p-leading:1.5;--ui-typography-margin-heading:.75rem;--ui-typography-margin-body:1.125rem;--ui-layout-container:1.25rem;--ui-layout-grid:3.625rem;--ui-layout-gutter:1rem;--ui-gap-cta:.75rem;--ui-gap-content:2rem;--ui-radius-button:5rem;--ui-radius-input:5rem}html{box-sizing:border-box}*,:after,:before{box-sizing:inherit}body{background-color:var(--ui-color-background-primary);color:var(--ui-color-typography-body);font-family:var(--ui-typography-typeface);font-feature-settings:"liga","kern";font-size:var(--ui-typography-p);font-weight:400;line-height:var(--ui-typography-p-leading);margin:0 auto;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased}a{color:var(--ui-color-brand);text-decoration:none}h1,h2,h3,p{margin-top:0}h1,h2,h3{color:var(--ui-color-typography-heading);margin-bottom:var(--ui-typography-margin-heading)}h1{font-size:var(--ui-typography-h1);line-height:var(--ui-typography-h1-leading)}h2{font-size:var(--ui-typography-h2);line-height:var(--ui-typography-h2-leading)}h3{font-size:var(--ui-typography-h3);line-height:var(--ui-typography-h3-leading)}p{margin-bottom:var(--ui-typography-margin-body)}p:last-child{margin-bottom:0}strong{font-weight:700}small{font-size:var(--ui-typography-s)}.ui-text-note{color:var(--ui-color-typography-note);line-height:1}img,svg{display:block;height:auto;margin:0 auto;max-width:100%}.ui-layout-container{padding-left:var(--ui-layout-container);padding-right:var(--ui-layout-container)}.ui-layout-flex,.ui-layout-grid{align-items:center;justify-content:center}.ui-layout-flex{display:flex}.ui-layout-grid{display:grid}.ui-component-cta{flex-direction:column;row-gap:var(--ui-gap-cta)}button,input{color:inherit;font-family:inherit;font-size:var(--ui-typography-p);line-height:1;margin:0;outline:0;text-rendering:inherit;text-transform:none}form{width:100%}.ui-component-form{background-color:var(--ui-color-form-input);border-radius:var(--ui-radius-input);grid-template-columns:minmax(0,1fr) auto;padding:.25rem}::placeholder{color:var(--ui-color-typography-note)}.ui-component-input{background-color:var(--ui-color-form-input);border:.0625rem solid var(--ui-color-form-input);border-radius:var(--ui-radius-input)}.ui-component-input-medium{height:2.5rem;padding:.625rem 1rem .75rem}button{background:0 0;border:0;cursor:pointer;display:block;padding:0}.ui-component-button{border:.0625rem solid var(--ui-color-brand);border-radius:var(--ui-radius-button);display:block;font-weight:700;line-height:1;text-align:center}.ui-component-button-primary{background-color:var(--ui-color-brand);color:var(--ui-color-typography-button)}.ui-component-button-medium{padding:.625rem .875rem .75rem;width:fit-content}.ui-section-header{padding-bottom:1.25rem;padding-top:1.25rem}.ui-section-header__layout{justify-content:space-between}.ui-section-content{padding-bottom:2em;padding-top:5rem;text-align:center}.ui-section-content--image{margin-bottom:var(--ui-gap-content);margin-top:var(--ui-gap-content)}.ui-section-content--feature{row-gap:var(--ui-gap-content)}.ui-section-content--icon{margin-bottom:1rem}.ui-section-close{padding-bottom:5rem;padding-top:5rem;text-align:center}.ui-section-footer{padding-bottom:1.25rem;padding-top:1.25rem}.ui-section-footer__layout{column-gap:var(--ui-layout-gutter)}.ui-section-footer--copyright{margin-bottom:0;margin-right:auto}@media screen and (min-width:48rem){:root{--ui-typography-h1:2.1875rem;--ui-typography-h2:1.75rem;--ui-typography-h3:1.4375rem;--ui-typography-p:1.125rem;--ui-typography-s:.875rem;--ui-typography-margin-body:1.25rem;--ui-layout-container:4.25rem;--ui-layout-gutter:1.5rem;--ui-gap-content:3rem}.ui-layout-column-center,.ui-layout-container{margin-left:auto;margin-right:auto}.ui-layout-grid-3{column-gap:var(--ui-layout-gutter);grid-template-columns:repeat(2,1fr);justify-items:center}.ui-layout-grid-3 div:last-of-type{left:calc(50% + (var(--ui-layout-gutter)/ 2));position:relative}.ui-layout-column-4{width:calc((var(--ui-layout-grid) * 4) + (var(--ui-layout-gutter) * 3))}.ui-layout-column-6{width:calc((var(--ui-layout-grid) * 6) + (var(--ui-layout-gutter) * 5))}.ui-section-header{padding-bottom:2rem;padding-top:2rem}.ui-section-content{padding-bottom:3rem}.ui-section-content--icon{height:4rem;width:4rem}.ui-section-footer{padding-bottom:2rem;padding-top:2rem}}@media screen and (min-width:64rem){:root{--ui-layout-container:0}a{transition:all 250ms ease}a:not(.ui-component-button):hover{color:var(--ui-color-typography-body)}.ui-layout-container{width:60rem}.ui-layout-grid-3{grid-template-columns:repeat(3,1fr)}.ui-layout-grid-3 div:last-of-type{position:static}}@media screen and (min-width:75rem){:root{--ui-typography-h1:2.75rem;--ui-typography-h2:2.1875rem;--ui-typography-h3:1.75rem;--ui-typography-h4:1.4375rem;--ui-typography-margin-heading:1rem;--ui-typography-margin-body:1.75rem;--ui-layout-grid:4rem;--ui-layout-gutter:2rem;--ui-gap-content:4rem}.ui-text-intro{font-size:var(--ui-typography-h4)}.ui-layout-container{width:70rem}.ui-section-header{padding-bottom:3rem;padding-top:3rem}.ui-section-content{padding-bottom:5rem;padding-top:7.5rem}.ui-section-content--icon{height:5rem;margin-bottom:1.125rem;width:5rem}.ui-section-close{padding-bottom:7.5rem;padding-top:7.5rem}.ui-section-footer{padding-bottom:3rem;padding-top:3rem}}:root{--material-color-yellow-50:#fffde7;--material-color-yellow-100:#fff9c4;--material-color-orange-500:#ff9800;--material-color-orange-600:#fb8c00;--base-background:#fff;--base-color:#333;--base-color-lighter:#777;--base-color-muted:#999;--yellow-background:var(--material-color-yellow-100);--yellow-background-lighter:var(--material-color-yellow-50);--orange-background-dark:var(--material-color-orange-500);--orange-background-dark-active:var(--material-color-orange-600);--hatena-color:#01a5df;--base-font:-apple-system,BlinkMacSystemFont,Helvetica Neue,Yu Gothic,YuGothic,Verdana,Meiryo,M+ 1p,sans-serif;--ui-gap-content:2em}.ui-text-note{color:var(--base-color-muted)}.ui-section-header__layout img{display:inline-block;width:24px;height:24px;vertical-align:middle}.ui-section-content{padding-top:2.5em;padding-bottom:3.5rem}.ui-section-header{padding-top:2rem;padding-bottom:1rem}.ui-component-form{border-radius:0;grid-template-columns:auto minmax(0,1fr) auto}.ui-component-form .ui-component-button{border-radius:0;background:var(--orange-background-dark);border-color:var(--orange-background-dark)}.ui-component-form .ui-component-button.active{background:var(--orange-background-dark-active);border-color:var(--orange-background-dark-active)}@media screen and (min-width:48rem){.ui-layout-grid-3 div:last-of-type{left:0}}@media screen and (min-width:75rem){.ui-layout-grid-3{grid-template-columns:repeat(4,1fr)}}.ui-typography-heading{text-align:left}.ui-typography-heading small{color:var(--base-color-muted)}img{color:var(--base-color-muted)}.ui-section-header__layout .ui-section-header__title{display:inline-block;line-height:22px;vertical-align:middle;font-weight:700;font-size:1.3em;color:var(--base-color)}.ui-top-section{padding-bottom:2em}.ui-component-form__label{margin-left:.2em}.ui-component-form__label img{width:32px;height:32px}.ui-component-form__label span{font-weight:700}.ui-top-section .ui-text-note{margin-bottom:.6em}.ui-top-section .ui-top-section__subscribe{margin-top:.3em;display:flex;gap:.5em}.ui-top-section .ui-top-section__subscribe img{height:37px;width:auto}.ui-section-nav__layout{justify-content:start}.ui-section-nav__link{font-weight:700;margin-right:1.5em;padding:.5em 0;border-bottom:2px solid transparent;color:var(--base-color-muted)}.ui-section-nav__link--active{color:var(--base-color);border-bottom-color:var(--base-color)}.ui-section-content__feed-date-heading{text-align:left;font-size:1.2em;color:var(--base-color-lighter);margin-top:1em;margin-bottom:1em;padding:.4em .3em;border-bottom:1px solid var(--base-color-lighter);position:sticky;top:0;z-index:1;background-color:var(--yellow-background-lighter)}.ui-section-feed{background:var(--yellow-background-lighter)}.ui-section-feed .ui-layout-grid{align-items:flex-start}.ui-section-feed .ui-text-note{text-align:left;font-size:.9em}.ui-container-feed{text-align:left;margin-top:1em;margin-bottom:2em;justify-items:left}.ui-container-feed.ui-container-feed--hot{margin-top:2em}.ui-feed-item{display:grid;color:var(--base-color);grid-template-columns:130px 1fr;align-content:start;grid-gap:0 0.5em}.ui-feed-item .ui-feed-item__og-image img{width:100%;height:auto;max-height:7em;object-fit:contain;object-position:center top}.ui-feed-item .ui-feed-item__title{font-weight:700;font-size:.9em;-webkit-line-clamp:3;-webkit-box-orient:vertical;display:-webkit-box;overflow:hidden;word-break:break-all}.ui-feed-item .ui-feed-item__title:hover{text-decoration:underline}.ui-feed-item .ui-feed-item__title:visited{color:var(--base-color-lighter)}.ui-feed-item .ui-feed-item__hatena-count{margin:.1em 0;font-size:.7em}.ui-feed-item .ui-feed-item__hatena-count img{display:inline;width:1.25em;height:1.25em;vertical-align:middle}.ui-feed-item .ui-feed-item__hatena-count span{color:var(--hatena-color);font-weight:700;vertical-align:middle}.ui-feed-item .ui-feed-item__blog-title{margin:.3em 0;font-size:.75em}.ui-feed-item .ui-feed-item__blog-title--link:hover{text-decoration:underline}.ui-feed-item .ui-feed-item__summary{font-size:.75em;margin:.3em 0;word-break:break-all;overflow:hidden;-webkit-line-clamp:2;-webkit-box-orient:vertical;display:-webkit-box;color:var(--base-color-muted)}.ui-feed-item .ui-feed-item__date{color:var(--base-color-muted);font-size:.7em}@media screen and (min-width:48rem){.ui-feed-item{display:block}.ui-feed-item .ui-feed-item__og-image{display:block}.ui-feed-item .ui-feed-item__og-image img{height:9em;max-height:9em}.ui-feed-item .ui-feed-item__title{margin-top:.5em}}@media screen and (min-width:75rem){.ui-feed-item .ui-feed-item__og-image img{height:8em;max-height:8em}}.ui-section-blog{background:var(--yellow-background-lighter)}.ui-container-blog{text-align:left;margin-top:2em}.ui-blog{display:grid;color:var(--base-color);grid-template-columns:130px 1fr;align-content:start;grid-gap:0 0.5em}.ui-blog .ui-blog__og-image img{width:100%;height:auto;max-height:7em;object-fit:contain;object-position:center top}.ui-blog .ui-blog__title{display:block;font-weight:700;word-break:break-all}.ui-blog .ui-blog__title:hover{text-decoration:underline}.ui-blog .ui-blog__link{display:block;font-size:.7em;word-break:break-all;overflow:hidden;margin:.2em 0}.ui-blog .ui-blog__link:hover{text-decoration:underline}.ui-blog .ui-blog__description{font-size:.75em;margin:.3em 0;word-break:break-all;overflow:hidden;-webkit-line-clamp:2;-webkit-box-orient:vertical;display:-webkit-box;color:var(--base-color-muted)}.ui-blog .ui-blog__date{color:var(--base-color-muted);font-size:.7em}@media screen and (min-width:48rem){.ui-blog{display:block}.ui-blog .ui-blog__og-image{display:block}.ui-blog .ui-blog__og-image img{width:auto;height:9em;max-height:9em}.ui-blog .ui-blog__title{margin-top:.5em}}@media screen and (min-width:75rem){.ui-blog .ui-blog__og-image img{width:auto;height:8em;max-height:8em}}.ui-container-blog-summary{text-align:left;margin-bottom:2em}.ui-blog-summary .ui-blog-summary__link{display:block;word-break:break-all;overflow:hidden;margin:.2em 0}.ui-blog-summary .ui-blog-summary__link:hover{text-decoration:underline}.ui-blog-summary .ui-blog-summary__description{margin:.3em 0;word-break:break-all;color:var(--base-color-muted)}.ui-section-footer .ui-section-footer__site-info{margin-bottom:1.5em;display:block;font-size:.9em}.ui-section-footer .ui-section-footer__site-info .ui-text-note{margin-bottom:.7em;line-height:1.4em}
</style><script async src="https://www.googletagmanager.com/gtag/js?id=G-"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-")</script><title>Zennの「LLM」のフィードのフィード｜生成AI・LLM関連テックブログRSS</title></head><body><header role="banner" class="ui-section-header"><div class="ui-layout-container"><div class="ui-section-header__layout ui-layout-flex"><a href="https://r-kagaya.github.io/llm-tech-blog-rss-feed/" role="link" aria-label="#"><img src="../../images/icon.png" alt="サイトロゴ" loading="lazy" decoding="async" width="96" height="96"> <span class="ui-section-header__title">生成AI・LLM関連テックブログRSS</span> </a><a href="https://github.com/r-kagaya/llm-tech-blog-rss-feed/" role="link" aria-label="#"><img src="../../images/github-mark.png" alt="GitHubロゴ" loading="lazy" decoding="async" width="96" height="96"></a></div></div></header><main role="main"><nav class="ui-nav"><div class="ui-layout-container"><div 
class="ui-section-nav__layout ui-layout-flex"><a class="ui-section-nav__link" href="../../">フィード</a> <a class="ui-section-nav__link" href="../../hot/">人気フィード</a> <a class="ui-section-nav__link" href="../../blogs/">ブログ一覧</a></div></div></nav><section class="ui-section-content ui-section-feed"><div class="ui-layout-container"><h2 class="ui-typography-heading">Zennの「LLM」のフィード</h2><div class="ui-container-blog-summary"><div class="ui-blog-summary"><a class="ui-blog-summary__link" href="https://zenn.dev/topics/llm">https://zenn.dev/topics/llm</a><p class="ui-blog-summary__description"></p></div></div><h3 class="ui-typography-heading">フィード</h3><div class="ui-section-content--feature ui-layout-grid ui-layout-grid-3 ui-container-feed ui-container-feed--no-image"><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/codeciao/articles/6d0a83e234a34a"><picture><source type="image/webp" 
srcset="../../images/feed-thumbnails/Cj5INdFQzg-150.webp 150w, ../../images/feed-thumbnails/Cj5INdFQzg-450.webp 450w" sizes="100vw"><source type="image/jpeg" srcset="../../images/feed-thumbnails/Cj5INdFQzg-150.jpeg 150w, ../../images/feed-thumbnails/Cj5INdFQzg-450.jpeg 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/Cj5INdFQzg-150.jpeg" width="450" height="236"></picture></a><div class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/codeciao/articles/6d0a83e234a34a">Clineに全部賭ける前に　〜Clineの動作原理を深掘り〜</a><div class="ui-feed-item__hatena-count" title="はてなブックマーク数"><img src="../../images/hatenabookmark-icon.png" alt="はてなブックマークアイコン" loading="lazy" decoding="async" width="96" height="96"> <span>2</span></div><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">
はじめに AIのコーディングアシスタントとして最近、急速に注目を集めているCline。VSCode上でAIと連携し、コード生成からバグ修正、さらにはターミナル操作まで自動化できるこのツールは、多くのエンジニアの生産性を劇的に向上させています。 mizchiさんの『CLINEに全部賭けろ』という記事では、 AIから引き出せる性能は、自分の能力にそのまま比例する AI自体を管理するパイプライン設計を自分のコアスキルにする必要がある ともあるように、エンジニアはClineという強力なツールの最大限を使えるようになっていくべきです。 「AIの上手な使い方」が今のエンジニアにとって必須スキ...</div><div class="ui-feed-item__date" title="2025-03-02 21:00:01">10時間前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/syfut/articles/85fbe2a719543f"><picture><source type="image/webp" srcset="../../images/feed-thumbnails/zidCaLeIo0-150.webp 150w, ../../images/feed-thumbnails/zidCaLeIo0-450.webp 450w" sizes="100vw"><source type="image/jpeg" srcset="../../images/feed-thumbnails/zidCaLeIo0-150.jpeg 150w, ../../images/feed-thumbnails/zidCaLeIo0-450.jpeg 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/zidCaLeIo0-150.jpeg" width="450" height="236"></picture></a><div 
class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/syfut/articles/85fbe2a719543f">ホームページをAI検索に最適化するには(AIO、LLMO、LLMs.txt)</a><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">AIはあなたのウェブサイトを本当に理解しているか？ ChatGPTやGemini、Claudeなどの大規模言語モデル（LLM）が日常的に使われるようになり、さらに検索機能やDeepResearchなどの機能が加わったことにより、AI自身がホームページ・Webサイトを探索・閲覧するのが普通になってきました。 これまで、ホームページはGoogle検索に効率的に引っ掛かるようSEO対策をすることが一般的でしたが、今後はAIに検索されやすくなるような工夫が必要になります。 それをLLMOやAIOと呼びます。 HTMLやCSS、JavaScriptが複雑に絡み合った現代のウェブサイトは、人間が...</div><div class="ui-feed-item__date" title="2025-03-02 19:00:48">12時間前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/hirayuki/articles/3e7271df613dd4"><picture><source type="image/webp" srcset="../../images/feed-thumbnails/WZiui6cqWA-150.webp 150w, ../../images/feed-thumbnails/WZiui6cqWA-450.webp 450w" sizes="100vw"><source type="image/jpeg" 
srcset="../../images/feed-thumbnails/WZiui6cqWA-150.jpeg 150w, ../../images/feed-thumbnails/WZiui6cqWA-450.jpeg 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/WZiui6cqWA-150.jpeg" width="450" height="236"></picture></a><div class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/hirayuki/articles/3e7271df613dd4">LLMOpsって何だ？: MLflow Tracing Conceptsを読み込む</a><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">Tracing Concepts 本記事はほぼほぼ MLflowの公式ドキュメントTracing Conceptsを読み進めているだけの記事ではありますが、ところどころわかりにくい箇所は実際にコードを実行して検証等もしているので、ぜひご覧ください。 What is tracing? あらためてtracingとは以下のように述べられています。 Tracing in the context of machine learning (ML) refers to the detailed tracking and recording of the data flow and proc...</div><div class="ui-feed-item__date" title="2025-03-02 14:44:44">16時間前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" 
href="https://zenn.dev/hirayuki/articles/6c0609a86e5766"><picture><source type="image/webp" srcset="../../images/feed-thumbnails/XMs1stciXf-150.webp 150w, ../../images/feed-thumbnails/XMs1stciXf-450.webp 450w" sizes="100vw"><source type="image/jpeg" srcset="../../images/feed-thumbnails/XMs1stciXf-150.jpeg 150w, ../../images/feed-thumbnails/XMs1stciXf-450.jpeg 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/XMs1stciXf-150.jpeg" width="450" height="236"></picture></a><div class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/hirayuki/articles/6c0609a86e5766">LLMOpsって何だ？: MLflow Tracing for LLM Observabilityを読み込む</a><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">
MLflow Tracing for LLM Observability MLflow TracingとはいわゆるLLMOps系の処理であり、GenAIを活用した Applicationにおいて promptとresponseを記録して監視していこうというツールです。 MLflowを中心に、というよりは、GenAI applicationの tracingという概念の理解を中心に取り組みます。 ちなみに実際にtracingを始めてみたい方は脳死のautologが非常に強力だと感じており、下記コードは実際に動作した内容になるので是非これで結果をいくつか取得してみてください。 impo...</div><div class="ui-feed-item__date" title="2025-03-02 13:03:55">18時間前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/hidaken/articles/60dc33d76f241e"><picture><source type="image/webp" srcset="../../images/feed-thumbnails/wzor2-XJtL-150.webp 150w, ../../images/feed-thumbnails/wzor2-XJtL-450.webp 450w" sizes="100vw"><source type="image/jpeg" srcset="../../images/feed-thumbnails/wzor2-XJtL-150.jpeg 150w, ../../images/feed-thumbnails/wzor2-XJtL-450.jpeg 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/wzor2-XJtL-150.jpeg" width="450" height="236"></picture></a><div 
class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/hidaken/articles/60dc33d76f241e">MCP × LLM：Confluenceに記載の企画書から要件定義・JIRAのPBI作成</a><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">はじめに 近年、大規模言語モデル（LLM）の進化により、さまざまな業務の自動化が可能になってきました。 特にソフトウェア開発のドキュメント作成や要件管理は、多くの時間とリソースを必要とする作業であり、効率化の余地が大きい領域です。 本記事では、Model Context Protocol（MCP）を活用して、企画書から要件定義書の作成、さらにはJiraのPBI（Product Backlog Item）作成までを自動化した内容を共有します。 本記事でわかること LLMとConfluence/Jiraを連携させる方法 企画書から要件定義書、PBIへの自動変換の実現方法 MC...</div><div class="ui-feed-item__date" title="2025-03-02 12:47:51">18時間前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/zenn_tkc/articles/e358f6f7c16143"><picture><source type="image/webp" srcset="../../images/feed-thumbnails/xV5kFJG7G9-150.webp 150w, ../../images/feed-thumbnails/xV5kFJG7G9-450.webp 450w" sizes="100vw"><source type="image/jpeg" 
srcset="../../images/feed-thumbnails/xV5kFJG7G9-150.jpeg 150w, ../../images/feed-thumbnails/xV5kFJG7G9-450.jpeg 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/xV5kFJG7G9-150.jpeg" width="450" height="236"></picture></a><div class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/zenn_tkc/articles/e358f6f7c16143">Cline開発に設計図を！Design Doc生成ステップを挟んでみる</a><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">TL;DR Clineを使って開発する際にまずDesign Docを生成させることを試してみる。 既存コードのリファクタリングを効率的に実施したい。 llm用のDesign Doc llm_design_doc_template.mdを用意してみた。 LLMはgemini-2.0-pro-exp-02-05 開発プロセス 以下の手順で開発を進めます。 LLM用のDesignDoc Template LLM用のdesign_docテンプレート llm_design_doc_template.md # 開発プロジェクト 設計ドキュメントテンプレート (design_doc)...</div><div class="ui-feed-item__date" title="2025-03-02 10:52:20">20時間前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" 
href="https://zenn.dev/shakshi3104/articles/488bcb006826ea"><picture><source type="image/webp" srcset="../../images/feed-thumbnails/jME05Ly4kE-150.webp 150w, ../../images/feed-thumbnails/jME05Ly4kE-450.webp 450w" sizes="100vw"><source type="image/jpeg" srcset="../../images/feed-thumbnails/jME05Ly4kE-150.jpeg 150w, ../../images/feed-thumbnails/jME05Ly4kE-450.jpeg 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/jME05Ly4kE-150.jpeg" width="450" height="236"></picture></a><div class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/shakshi3104/articles/488bcb006826ea">MicrosoftのSLM「Phi-4-mini-instruct」をM2 Pro Mac miniで動かしてみた</a><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">
Phi-4-mini-instructとは 「Phi-4-mini-instruct」の前に、まず「Phi-4」について簡単に紹介します。 Phi-4は、Microsoftによって開発されたパラメータ数が14Bの小規模言語モデル(SLM)です。2025年1月からMITライセンスとなったため、商用利用も可能なモデルです。Phi-4-mini-instructは、パラメータ数が3.8Bとさらに少なくなったSLMで2025年2月に公開されました。 https://techcommunity.microsoft.com/blog/aiplatformblog/introducing-phi-...</div><div class="ui-feed-item__date" title="2025-03-02 10:30:01">21時間前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/ayutaso/articles/caa23d10b2cead"><picture><source type="image/webp" srcset="../../images/feed-thumbnails/TJ7plpFJT4-150.webp 150w, ../../images/feed-thumbnails/TJ7plpFJT4-450.webp 450w" sizes="100vw"><source type="image/jpeg" srcset="../../images/feed-thumbnails/TJ7plpFJT4-150.jpeg 150w, ../../images/feed-thumbnails/TJ7plpFJT4-450.jpeg 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/TJ7plpFJT4-150.jpeg" width="450" height="236"></picture></a><div 
class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/ayutaso/articles/caa23d10b2cead">【LLM】モデルの重みをWebLLM形式に変換する方法</a><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">LLMをブラウザで推論させる方法にwebLLMを使う方法がある。webLLMを使うとWASMとWebGPUを使用して量子化モデルをブラウザで動かすことが出来る。 WebLLM は、ハードウェアアクセラレーションを使用して言語モデル推論を Web ブラウザーに直接提供する、高性能なブラウザー内 LLM 推論エンジンです。すべてがサーバーのサポートなしでブラウザー内で実行され、WebGPU で高速化されます。 しかし、web-llmではwebGPU用の変換をする必要があるので、ここではモデルをwebGPU用にMLC-LLMに変換する方法を紹介する。 やったこと 独自でfine-tun...</div><div class="ui-feed-item__date" title="2025-03-02 06:51:26">1日前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/ryoushin/articles/63fa80d47df8db"><picture><source type="image/webp" srcset="../../images/feed-thumbnails/89H4hxKRLe-150.webp 150w, ../../images/feed-thumbnails/89H4hxKRLe-450.webp 450w" sizes="100vw"><source type="image/jpeg" 
srcset="../../images/feed-thumbnails/89H4hxKRLe-150.jpeg 150w, ../../images/feed-thumbnails/89H4hxKRLe-450.jpeg 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/89H4hxKRLe-150.jpeg" width="450" height="236"></picture></a><div class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/ryoushin/articles/63fa80d47df8db">macOSのSafariでGrokを使うときの日本語入力問題を解決する方法</a><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">はじめに macOSのSafariで日本語入力をしながらGrok（grok.com）を使っている方は、こんな問題に悩まされたことはありませんか？ 日本語入力中にEnterキーで文字を確定したら、入力途中のメッセージが送信されてしまう 変換候補を選んだ直後に、意図せずメッセージが送信される 思考を整理しながら入力できず、いちいち文章を別のエディタで作成してからコピペしている これはmacOSの日本語IMEと、Webアプリケーションのイベント処理の相性問題です。特にAIチャットサービスでは、Enterキーが「送信」と「日本語確定」の両方に使われることで起こる問題です。この記事では、T...</div><div class="ui-feed-item__date" title="2025-03-02 02:32:01">1日前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" 
href="https://zenn.dev/yurikomium/articles/73b7f5863ed613"><picture><source type="image/webp" srcset="../../images/feed-thumbnails/5wMkh48iKv-150.webp 150w, ../../images/feed-thumbnails/5wMkh48iKv-450.webp 450w" sizes="100vw"><source type="image/jpeg" srcset="../../images/feed-thumbnails/5wMkh48iKv-150.jpeg 150w, ../../images/feed-thumbnails/5wMkh48iKv-450.jpeg 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/5wMkh48iKv-150.jpeg" width="450" height="236"></picture></a><div class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/yurikomium/articles/73b7f5863ed613">初めてのRAGアプリケーション開発</a><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">
これまでLLMについて学んできたものの、ゼロから自分でRAGアプリケーションを作成したことがなかったので、今回初めて開発してみました。 Next.js + ベクトルDB + LlamaでシンプルなRAGアプリを構築したので、メモを残しておきます✍️ RAGが気になるけれどまだつくったことがない、という私と似た状況の方の参考になれば幸いです！ 香水RAGシステムの全体像 テーマに選んだのは「香水」。いくつかの条件を入れたら香水を探せるアプリケーションをつくることにしました。 香水に関する情報をベクトルデータベースに格納し、ユーザーの質問に対して関連性の高い情報を検索・提供するRAG（R...</div><div class="ui-feed-item__date" title="2025-03-02 02:09:50">1日前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/19931/articles/06a979996fac92"><picture><source type="image/webp" srcset="../../images/feed-thumbnails/TNYPTKHIPM-150.webp 150w, ../../images/feed-thumbnails/TNYPTKHIPM-450.webp 450w" sizes="100vw"><source type="image/jpeg" srcset="../../images/feed-thumbnails/TNYPTKHIPM-150.jpeg 150w, ../../images/feed-thumbnails/TNYPTKHIPM-450.jpeg 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/TNYPTKHIPM-150.jpeg" width="450" height="236"></picture></a><div 
class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/19931/articles/06a979996fac92">今更だけど DeepSeek R1 使ってみた（Ollama on Jetson）</a><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">はじめに この記事は LLM の性能評価ではなく、セットアップの記録です。 動作条件は以前の記事の通りで、Ollama を前提にしています。 https://zenn.dev/19931/articles/c6a2e36bf000d1 DeepSeek R1 は日本語のファインチューニングをしなくても、用途によっては充分な日本語性能が得られるようです。セットアップやパラメータ調整の手間を考えると「オリジナルモデルでいいや」という感想です。 日本語チューニングモデル セットアップ lightblue/DeepSeek-R1-Distill-Qwen-7B-Japanese を使...</div><div class="ui-feed-item__date" title="2025-03-02 01:47:03">1日前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/woniu/articles/e0ddfe2a300b49"><picture><source type="image/webp" srcset="../../images/feed-thumbnails/V9S_6ybKoL-150.webp 150w, ../../images/feed-thumbnails/V9S_6ybKoL-450.webp 450w" sizes="100vw"><source type="image/jpeg" 
srcset="../../images/feed-thumbnails/V9S_6ybKoL-150.jpeg 150w, ../../images/feed-thumbnails/V9S_6ybKoL-450.jpeg 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/V9S_6ybKoL-150.jpeg" width="450" height="236"></picture></a><div class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/woniu/articles/e0ddfe2a300b49">Electronでコード分析ツールを作りました</a><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">はじめに 開発者として、皆さんもこのような状況に直面したことがあるでしょう：ドキュメントのない古いプロジェクトを引き継ぎ、何万行もの見知らぬコードを前にした時の無力感。あるいは、自分で書いたコードでも、数ヶ月後に見返すと、まるで暗号を読むような感覚... こういった日常的な問題がきっかけとなり、CodeAskを開発しました。ElectronとLLMをベースにしたコード分析ツールで、複雑なコードベースを理解し、メンテナンスする際の困難を解決することを目指しています。 CodeAskの主な機能 インテリジェントなコード分析：LLM APIを活用し、コードベースを分析してレポー...</div><div class="ui-feed-item__date" title="2025-03-01 17:10:48">2日前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" 
href="https://zenn.dev/nwn/articles/7f01bdea71e939"><picture><source type="image/webp" srcset="../../images/feed-thumbnails/H6y-QO-k8B-150.webp 150w, ../../images/feed-thumbnails/H6y-QO-k8B-450.webp 450w" sizes="100vw"><source type="image/jpeg" srcset="../../images/feed-thumbnails/H6y-QO-k8B-150.jpeg 150w, ../../images/feed-thumbnails/H6y-QO-k8B-450.jpeg 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/H6y-QO-k8B-150.jpeg" width="450" height="236"></picture></a><div class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/nwn/articles/7f01bdea71e939">結局MCPって何に使えるの？おすすめサーバーの紹介</a><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">
そもそもMCPとは？ MCP は、アプリケーションが LLM にコンテキストを提供する方法を標準化するオープンプロトコルです。MCP を AI アプリケーション向けの USB-C ポートのようなものと考えてください。USB-C がデバイスをさまざまな周辺機器やアクセサリーに接続するための標準化された方法を提供するように、MCP は AI モデルを異なるデータソースやツールに接続するための標準化された方法を提供します。 MCPサーバーの紹介 今更感もありますが公式リポジトリで紹介されているMCPサーバーの中からいくつかピックアップしてそのユースケースについて紹介したいと思います...</div><div class="ui-feed-item__date" title="2025-03-01 03:59:56">2日前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/nakano_teppei/articles/7dbda321967965"><picture><source type="image/webp" srcset="../../images/feed-thumbnails/Q2FISlnDOe-150.webp 150w, ../../images/feed-thumbnails/Q2FISlnDOe-450.webp 450w" sizes="100vw"><source type="image/jpeg" srcset="../../images/feed-thumbnails/Q2FISlnDOe-150.jpeg 150w, ../../images/feed-thumbnails/Q2FISlnDOe-450.jpeg 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/Q2FISlnDOe-150.jpeg" width="450" height="236"></picture></a><div 
class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/nakano_teppei/articles/7dbda321967965">MCMCと変分推論（VI）は最先端のLLMでどう使われているのか？</a><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">1. はじめに 近年、大規模言語モデル（LLM: Large Language Models） の発展により、AIは自然言語処理（NLP）の分野で驚異的な進化を遂げています。GPTシリーズやLLaMA、PaLMなどの最先端LLMでは、確率的推論が重要な役割を果たします。 そこで、本記事では、MCMC（マルコフ連鎖モンテカルロ法）と変分推論（VI: Variational Inference）がLLMのトレーニングや最適化にどのように活用されているか を解説します。 2. LLMの学習と推論における確率的手法の必要性 LLMの学習は基本的に確率モデルを扱います。例えば、次のような...</div><div class="ui-feed-item__date" title="2025-03-01 03:56:47">2日前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/fleagne/articles/43f40c410e56a7"><picture><source type="image/webp" srcset="../../images/feed-thumbnails/KInH8YBasv-150.webp 150w, ../../images/feed-thumbnails/KInH8YBasv-450.webp 450w" sizes="100vw"><source type="image/jpeg" 
srcset="../../images/feed-thumbnails/KInH8YBasv-150.jpeg 150w, ../../images/feed-thumbnails/KInH8YBasv-450.jpeg 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/KInH8YBasv-150.jpeg" width="450" height="236"></picture></a><div class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/fleagne/articles/43f40c410e56a7">1歩1歩理解する、Embedding</a><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">はじめに 社内勉強会の資料を兼ねてアウトプット。発表までに何度か更新入れます。 検証リポジトリはこちら。 https://github.com/fleagne/embedding Claude 3.7 Sonnetによる、Embeddingまとめ https://claude.ai/share/3704b5b2-e867-4a23-ab08-bbea79222f18 すごいですね。このプロンプトでもここまで作成してくれる。 構成のベースラインとして、Claudeが書いてくれたものを深堀することで効率よくキャッチアップができそうですね。 ここで書いてくれたことを、掘り下げていきまし...</div><div class="ui-feed-item__date" title="2025-03-01 01:34:45">2日前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" 
href="https://zenn.dev/shosuke_13/articles/21d304b5f80e00"><picture><source type="image/webp" srcset="../../images/feed-thumbnails/dko20eHWAt-150.webp 150w, ../../images/feed-thumbnails/dko20eHWAt-450.webp 450w" sizes="100vw"><source type="image/jpeg" srcset="../../images/feed-thumbnails/dko20eHWAt-150.jpeg 150w, ../../images/feed-thumbnails/dko20eHWAt-450.jpeg 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/dko20eHWAt-150.jpeg" width="450" height="236"></picture></a><div class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/shosuke_13/articles/21d304b5f80e00">MLflow Prompt Engineering UIで国産モデルを活用：LLM-jpとの連携ガイド</a><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">
注意書き: 本記事の執筆には誤字脱字などの文章校正以外に 生成AIを利用していません。 MLflowは機械学習のMLOpsのためのOSSライブラリです。最新のバージョンでは、LLMに関する実験管理の機能が随時開発・リリースされています。今回はその中でもPrompt Engineering UIを使って、非サポートのローカルLLMを利用する方法を紹介します。 Prompt Enginnering UIとは？ MLflow Prompt Enginnering UIは、ノーコードで様々なモデルのプロンプトチューニングを行い、それらをUI上で簡単に比較することができる機能です。Prompt...</div><div class="ui-feed-item__date" title="2025-02-28 18:34:45">3日前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/headwaters/articles/ccfa133c76c6f5"><picture><source type="image/webp" srcset="../../images/feed-thumbnails/wmG7m-vb8W-150.webp 150w, ../../images/feed-thumbnails/wmG7m-vb8W-450.webp 450w" sizes="100vw"><source type="image/jpeg" srcset="../../images/feed-thumbnails/wmG7m-vb8W-150.jpeg 150w, ../../images/feed-thumbnails/wmG7m-vb8W-450.jpeg 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/wmG7m-vb8W-150.jpeg" width="450" height="236"></picture></a><div 
class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/headwaters/articles/ccfa133c76c6f5">プロンプトエンジニアリング - ReActに関して</a><div class="ui-feed-item__hatena-count" title="はてなブックマーク数"><img src="../../images/hatenabookmark-icon.png" alt="はてなブックマークアイコン" loading="lazy" decoding="async" width="96" height="96"> <span>1</span></div><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">背景 LLMマルチエージェントを使ったQAシステムのエージェント設計において、しばしば以下の課題が浮上します。 ユーザーとエージェントの会話の履歴を参考にせず、ユーザーの単発の質問に対して回答してしまう 決め打ちのルールやフローに基づいて回答を行わせるため、急に話題が変わった質問や想定外の質問に弱い 計算を間違える ハルシネーション（それっぽい嘘）が発生する ユーザーからの不明瞭、もしくは抽象的な質問に対して、自身で何の情報が足りないか補えない これらをどうにか解消できないか調べた結果、「ReAct」という手法にたどり着きました。 ReActとは 2022年に発表された論文「...</div><div class="ui-feed-item__date" title="2025-02-28 14:59:58">3日前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/satosh/articles/ca03eaa6bf326b"><picture><source type="image/webp" 
srcset="../../images/feed-thumbnails/1e30k9e7Rp-150.webp 150w, ../../images/feed-thumbnails/1e30k9e7Rp-450.webp 450w" sizes="100vw"><source type="image/jpeg" srcset="../../images/feed-thumbnails/1e30k9e7Rp-150.jpeg 150w, ../../images/feed-thumbnails/1e30k9e7Rp-450.jpeg 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/1e30k9e7Rp-150.jpeg" width="450" height="236"></picture></a><div class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/satosh/articles/ca03eaa6bf326b">Colab上でローカルLLMの簡易APIサーバーを構築する【Colab × FastAPI × ngrok】</a><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">
ああ〜、ローカル LLM のデモアプリを手軽に動かせるような環境を用意したいんじゃ〜 0. 想定読者 &amp; 本記事ハイライト 本記事では、Google Colab 上で GPU リソースを活用し、FastAPI と ngrok を用いてローカル LLM デモアプリを外部からアクセス可能な API サーバーとして構築する方法をご紹介します。 対象とする読者は、 LLM を用いた簡易 API サーバーを構築したい方 ローカル LLM のデモアプリを作成したいが、オンプレやクラウドサーバー環境整備に悩んでいる初心者〜中級者 簡易なプロトタイプを短時間で立ち上げたいエンジニア です...</div><div class="ui-feed-item__date" title="2025-02-28 12:33:21">3日前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/trysmr/articles/vscode_cline"><picture><source type="image/webp" srcset="../../images/feed-thumbnails/D9hYpG8Oh6-150.webp 150w, ../../images/feed-thumbnails/D9hYpG8Oh6-450.webp 450w" sizes="100vw"><source type="image/jpeg" srcset="../../images/feed-thumbnails/D9hYpG8Oh6-150.jpeg 150w, ../../images/feed-thumbnails/D9hYpG8Oh6-450.jpeg 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/D9hYpG8Oh6-150.jpeg" width="450" height="236"></picture></a><div 
class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/trysmr/articles/vscode_cline">ClineとAIエージェント時代のプログラミングに関する所感</a><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">はじめに そろそろAIエージェントを使った開発に慣れる必要性を感じてClineを試した。ClineはVS Code上で動作する対話型のAIエージェントで、コードの生成、修正、テストなどを自律的に行うことができる。GitHub Copilot（以下Copilot）の使用感から大きな期待はなかったものの、コード生成の精度や自律的な動作など、予想を大きく超える体験となった。その所感をまとめる（Xでポストした内容のまとめも兼ねている）。 使用ツール 本家Clineを採用。派生版も存在するがオリジナルで。 https://github.com/cline/cline バージョンは3.4.9...</div><div class="ui-feed-item__date" title="2025-02-28 11:53:40">3日前</div></div></div><div class="ui-feed-item"><a class="ui-feed-item__og-image" href="https://zenn.dev/fkky/articles/c55394df4801ed"><picture><source type="image/webp" srcset="../../images/feed-thumbnails/Z2SoUqAzA8-150.webp 150w, ../../images/feed-thumbnails/Z2SoUqAzA8-450.webp 450w" sizes="100vw"><source type="image/jpeg" 
srcset="../../images/feed-thumbnails/Z2SoUqAzA8-150.jpeg 150w, ../../images/feed-thumbnails/Z2SoUqAzA8-450.jpeg 450w" sizes="100vw"><img alt="記事のアイキャッチ画像" loading="lazy" decoding="async" src="../../images/feed-thumbnails/Z2SoUqAzA8-150.jpeg" width="450" height="236"></picture></a><div class="ui-feed-item__content"><a class="ui-feed-item__title" href="https://zenn.dev/fkky/articles/c55394df4801ed">【LangGraphの教科書】OpenDeepResearchの実装がとても参考になります</a><div class="ui-feed-item__blog-title">Zennの「LLM」のフィード</div><div class="ui-feed-item__summary">ここから見てね https://github.com/langchain-ai/open_deep_research はじめに はじめまして、ふっきーです。 最近話題のDeepResearchの中身が気になったので、LangChainのGitにあるOpenDeepResearchのコードを読んでみました。 こんな感じのやつ DeepResearchの実現方法としては、ふーんという感じでしたが、 LangGraphの実装として見るとさまざまな要素があり、非常に勉強になりました。 AIワークフローを開発するうえで有益だなと思ったので、共有します。 LangGraphを使って、 AIワ...</div><div class="ui-feed-item__date" title="2025-02-28 10:12:56">3日前</div></div></div></div></div></section></main><footer role="contentinfo" 
class="ui-section-footer"><div class="ui-layout-container"><div class="ui-layout-column-6 ui-layout-column-center"><div class="ui-component-cta ui-layout-flex ui-section-footer__site-info"><p class="ui-text-note">このサイトは<br>記事を読んでその企業の技術・カルチャーを知れることや<br>質の高い技術情報を得られることを目的としています。</p><p class="ui-text-note">追加したいブログがある場合は<br><a href="https://github.com/r-kagaya/llm-tech-blog-rss-feed#%E3%82%B5%E3%82%A4%E3%83%88%E3%81%AE%E8%BF%BD%E5%8A%A0%E6%96%B9%E6%B3%95">サイトの追加方法</a> をご参照ください。</p></div></div></div><div class="ui-layout-container"><div class="ui-section-footer__layout ui-layout-flex"><p class="ui-section-footer--copyright ui-text-note"><a class="ui-text-note" href="https://github.com/r-kagaya/"><small>@r-kagaya</small></a></p><a href="https://github.com/r-kagaya/llm-tech-blog-rss-feed/" role="link" aria-label="#" class="ui-text-note"><small>GitHub</small></a></div></div></footer></body></html>